---
title: "Depression Score Prediction Analysis"
author: "Toheeb"
date: "`r Sys.Date()`"
output: rmarkdown::github_document
analysis_goal: "to predict depression score among US adults"
method: "linear regression"
data_source: "NHANES 2017-March 2020 data"
---
```{r Setup & package loading}
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")

pacman::p_load(
  here, haven, tidyverse, mice, rsample, naniar, skimr, recipes, parsnip, workflows, tune, yardstick, broom, GGally, glmnet, dials, doParallel, car, lmtest, sessioninfo
)
```


# Context & Goal
This notebook builds and evaluates linear regression models predicting a depression score derived from DPQ items (PHQ-9 depression scale) in NHANES 2017-March 2020 data. It is annotated with explanations about modeling choices, reproducible preprocessing using recipes, diagnostic checks, and a short example of hyperparameter tuning (elastic net) to illustrate regularization for predictive modeling.

# Exploratory Data Anlysis
## Data Import
**Note on file paths** — it's best practice to avoid absolute Windows paths in notebooks that may be shared. So I place data in `data/` and use `here::here...`. This allows others to run the notebook without having to edit paths.
```{r}
depression <- read_dta(here::here("data", "depression.dta"))
demographics <- read_dta(here::here("data", "demographics.dta"))
physical_activity <- read_dta(here::here("data", "physical-activity.dta"))
drinking <- read_dta(here::here("data", "alcohol-use.dta"))
smoking <- read_dta(here::here("data", "cigarette-use.dta"))
bmi <- read_xpt(here::here("data", "p_bmx.xpt"))
sleeping <- read_dta(here::here("data", "sleep-disorder.dta"))
```


Quick checks to confirm the ID column has the same name across all data frames:
```{r}
names(depression)[1]; names(demographics)[1]; names(physical_activity)[1]; 
names(drinking)[1]; names(smoking)[1]; names(bmi)[1]; names(sleeping)[1];
```


The column name is lowercase in all data frames, except `bmi`. So, I fix this:
```{r}
bmi <- bmi %>%
  rename(seqn = SEQN)
```

Next, I check whether each data frame has any duplicate IDs.
```{r}
# create a list of the data frames
data_list = list(dep = depression, dem = demographics, phys_act = physical_activity, 
                 drink = drinking, smoking = smoking, bmi = bmi, sleep = sleeping)

# use lapply to find duplicate IDs (seqn) in each data frame
duplicate_IDs <- lapply(data_list, function(df) {
  df %>%
    count(seqn) %>%
    filter(n>1)
})

print(duplicate_IDs)
```


## Compute depression sum score (from the DPQ variables)
Click [here](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DPQ_J.htm#DPQ100) for description of the DPQ variables. In the following steps, I compute a summed score from the nine DPQ items. 

* 1: select the variables that constitute the depression score (the PHQ-9 scale)
```{r}
dpq_vars <- c("dpq010","dpq020","dpq030","dpq040","dpq050","dpq060","dpq070","dpq080","dpq090")
```

* 2: recode values 7 (Refused) and 9 (Don't know) as missing
```{r}
depression <- depression %>% 
  mutate(across(all_of(dpq_vars), ~ na_if(.x, 7) %>% 
                  na_if(9)))
```

* 3: check missingness patterns across the 9 items
```{r}
# calculate the percentage missingness across the 9 vars
missing_percentage_table <- depression %>%
  summarise(across(all_of(dpq_vars),
                   ~ sum(is.na(.)) / n()*100)) %>%
  tidyr::pivot_longer(
    cols = everything(),
    names_to = "variable",
    values_to = "Percent_Missing"
  )
missing_percentage_table
```
About 7% data is missing per column.

```{r}
# calculate the average percentage of missing items per respondent
row_missingness <- depression %>%
  mutate(missing_prop = rowMeans(is.na(across(all_of(dpq_vars))))) %>%
  summarise(avg_row_missingness = mean(missing_prop, na.rm=TRUE) * 100)

row_missingness
```

On average, each respondent is missing 7.4% of the 9 items (i.e., ≈0.67 items per respondent).


* 4: Person-mean imputation if <=2 missing items
```{r}
depression_cleaned <- depression %>%
  rowwise() %>%
  mutate(
    dpq_nonmiss = sum(!is.na(c_across(all_of(dpq_vars)))),
    dpq_mean    = mean(c_across(all_of(dpq_vars)), na.rm = TRUE),
    dep_score   = if (dpq_nonmiss >= 7) {     # require at least 7 of 9 items answered
                     sum(replace_na(c_across(all_of(dpq_vars)), dpq_mean))
                   } else {
                     NA_real_
                   }
  ) %>%
  ungroup() %>%
  filter(!is.na(dep_score)) %>% 
  dplyr::select(seqn, dep_score)
```

Why use person-mean imputation? Following established PHQ-9 scoring procedures, this approach is suitable when the missingness is below 20%. Participants with fewer than 3 missing PHQ-9 items had missing values replaced using individual mean imputation. Cases with 3 or more missing items were excluded from depression score computation. For more on person-mean imputation in PHQ-9 scoring, see [Rezvan et al.,2022](https://pmc.ncbi.nlm.nih.gov/articles/PMC9718541/) and [Shrive et al., 2006](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-6-57).




## Demographics — cleaning & factor creation
Click [here](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.htm) for description of the demographic variables. I take the relevant variables, handle refused/don't know values, and convert to labeled factors. 
```{r}
# Subset variables of interest
demographics <- demographics %>% 
  dplyr::select(seqn, riagendr, ridageyr, ridreth3, dmdeduc2, dmdmartz)
```

```{r}
# Rename to more convenient names
demographics <- demographics %>% 
  rename(sex = riagendr, age = ridageyr, race_eth = ridreth3, educ = dmdeduc2, marital = dmdmartz)
```


```{r}
# Recode refused/don't know codes to NA (per codebook)
demographics <- demographics %>% 
  mutate(educ = na_if(educ, 7),
         educ = na_if(educ, 9),
         marital = na_if(marital, 77),
         marital = na_if(marital, 99)
         )
```



```{r}
# We map them to compact numeric indices for easier factor creation
race_map <- function(x){
  case_when(
    x == 1 ~ "MexAmerican",
    x == 2 ~ "OtherHispanic",
    x == 3 ~ "NHWhite",
    x == 4 ~ "NHBlack",
    x == 6 ~ "NHAsian",
    x == 7 ~ "Other",
    TRUE ~ NA_character_
  )
  }

demo_recode <- demographics %>% mutate(race_eth = race_map(race_eth))
```


```{r}
# Convert to factors with chosen reference levels.
demographics_cleaned <- demo_recode %>% 
  mutate(
    sex = factor(sex, levels = c(1,2), labels = c("M","F")),
    race_eth = factor(race_eth,
                      levels = c("NHWhite","MexAmerican","OtherHispanic","NHBlack","NHAsian","Other")),
    educ = case_when(educ == 1 ~ "<9th", educ == 2 ~ "9-11th", educ == 3 ~ "HS/GED", 
                     educ == 4 ~ "SomeCollege", educ == 5 ~ ">=College", TRUE ~ NA_character_) %>%
      factor(., levels = c("HS/GED","<9th","9-11th","SomeCollege", ">=College")),
    marital = case_when(marital == 1 ~ "Married/LiveWithPartner", marital == 2 ~ "Wid/Div/Sep", 
                        marital == 3 ~ "NeverMarried", TRUE ~ NA_character_) %>%
      factor(., levels = c("Married/LiveWithPartner","Wid/Div/Sep","NeverMarried"))
)
```

Note: the ordering of variable levels in `levels=c(...)` is important, as the first level will be the reference level when one_hot argument is set to False (later in the `step_dummy()` function).

```{r}
# Quick checks for the cleaned demographics data
sapply(demographics_cleaned, class)
head(demographics_cleaned)
dim(demographics_cleaned)
```


## Select and clean other variables (physical activity, alcohol, smoking, BMI, sleep)

* Physical activity: Click [here](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/PAQ_J.htm) for description of physical activity variables.
```{r}
physical_activity <- physical_activity %>% 
  dplyr::select(seqn, paq665) %>% #paq665: moderate-level physical activity
  rename(physact = paq665) %>%
  mutate(physact = case_when(physact == 7 ~ NA_real_, physact == 9 ~ NA_real_, 
                             TRUE ~ as.numeric(physact)),
    physact = factor(physact, levels = c(1,2), labels = c("Active","Inactive")))
```


* Alcohol use: average number drinks per day. Click [here](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/ALQ_J.htm) for the variable's description.
```{r}
drinking <- drinking %>% 
  dplyr::select(seqn, alq130) %>% #alq130: Average number of alcoholic drinks per day in the past 12 mos
  rename(alc_drinks = alq130) %>%
  mutate(alc_drinks = if_else(alc_drinks %in% c(777, 999), NA_real_, as.numeric(alc_drinks)))
```


* Cigarette use: Click [here](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/SMQ_J.htm) for the variable description
```{r}
smoking <- smoking %>% 
  dplyr::select(seqn, smq020) %>% #smq020: whether respondent has ever smoked at least 100 cigarettes
  rename(cig100 = smq020) %>%
  mutate(cig100 = case_when(cig100 == 7 ~ NA_real_, cig100 == 9 ~ NA_real_, TRUE ~ as.numeric(cig100)),
         cig100 = factor(cig100, levels = c(1,2), labels = c("Yes","No")))
```


* Sleep: Click 
[here](https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/SLQ_J.htm) for variable description.
```{r}
sleeping <- sleeping %>% 
  dplyr::select(seqn, sld012) %>% #sld012: Number of hours of sleep daily
  rename(sleep_hrs = sld012) %>% 
  mutate(sleep_hrs = as.numeric(sleep_hrs))
```

* BMI (body mass index, measured in kg/m^2^)
```{r}
bmi <- bmi %>% 
  dplyr::select(seqn, BMXBMI) %>% 
  rename(bmi = BMXBMI) %>% 
  mutate(bmi = as.numeric(bmi))
```


```{r}
# Head checks
head(physical_activity); head(drinking); head(smoking); head(sleeping); head(bmi)
```



## Merge: anchor to demographics using `left_join`
Using `left_join()` anchored to `demographics` avoids duplicating rows unexpectedly (which can happen with full_join when datasets have different sampling frames). I then check how many rows and missingness are there.
```{r}
base <- demographics_cleaned %>% 
  distinct(seqn, .keep_all = TRUE)

data_list <- list(depression_cleaned, physical_activity, drinking, smoking, sleeping, bmi)

nhanes_combined <- reduce(data_list, left_join, by="seqn", .init = base)
```


```{r}
# How many rows and completeness
nrow(base); nrow(nhanes_combined) # compares number of rows in the base and final tables 
nhanes_combined %>% 
  map(~ sum(is.na(.)))
```

```{r}
# Quick glimpse
glimpse(nhanes_combined)
```



## Missingness exploration
Here, I inspect missingness and decide on an imputation strategy rather than immediate listwise deletion. I'll show the effect of `drop_na()` for transparency but avoid using it for modeling; instead I'll use recipe-based imputation.

```{r}
# Visualize missingness
vis_miss(nhanes_combined)
```

```{r}
# Show how many rows would remain if we dropped any NA
n_before <- nrow(nhanes_combined)
n_after <- nrow(nhanes_combined %>% 
                  drop_na())
cat('Rows before:', n_before, 'rows; After drop_na:', n_after,'\n Percentage kept:', round(n_after/n_before*100,1), '%')
```

The above comparison ssuggests that using `drop_na()` would result in over 65% data loss.

```{r}
# Summary of numeric distributions (non-missing)
skimr::skim(nhanes_combined)
```

For modeling, we'll illustrate median/mode imputation via recipes applied on training data.


## Exploratory numeric checks & correlation


```{r}
# Correlation among numeric columns
nhanes_combined %>% 
  dplyr::select(where(is.numeric)) %>% 
  GGally::ggpairs()
```

```{r}
# Dependent variable distribution
ggplot(nhanes_combined, aes(dep_score)) + geom_density(na.rm = TRUE) + labs(title = 'Depression score distribution')
```

Results: depression score is not normally distributed


# Modeling: consistent pattern for all models
**Plan**

1. Create a consistent `recipe` that includes imputation and dummy encoding. Apply `prep()` only on training data.
2. Fit models using `workflows` so preprocessing is encapsulated with model objects.
3. Evaluate on held-out test set (predictions -> metrics).
4. Run diagnostics on training fit and compare CV results for model selection.

## Model A — OLS on raw outcome (for baseline)
Establish a baseline using Ordinary Least Squares on the raw depression score. Includes standard preprocessing, diagnostics, and evaluation on held-out test data.

### Train/test split
```{r}
# For reproducibility, create a train/test split on the combined dataset but remove rows missing the outcome variable
analysis_df <- nhanes_combined %>% 
  filter(!is.na(dep_score))

set.seed(58)
split <- initial_split(analysis_df, prop = 3/4) #Assign 75% of the data as training set
train <- training(split)
test <- testing(split)

nrow(train); nrow(test)
```

That is, the analytical dataset, `analysis_df` (which is split into `train` and `test`) has a combined 8,299 rows. For context, using the `drop_na()` from earlier would have retained only 5,500 rows. 

### Recipe: imputation + dummies (applied to training set only)  
Median-impute numeric predictors, mode-impute categorical vars, one-hot encode factors, and remove zero-variance features.

```{r}
# Build a recipe: median impute numeric predictors, mode impute categorical, then dummy encode
dep_recipe <- recipe(dep_score ~ sex + age + race_eth + educ + marital + physact + alc_drinks + cig100 + bmi + sleep_hrs, data = train) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = FALSE) %>% #one_hot=FALSE: omits the first dummy category (to serve as reference category) in each categorical predictor. In anticipation of this, the desired reference group was placed first when defining levels (earlier in the EDA section)
  step_zv(all_predictors()) # remove zero-variance predictors if any
```

The following code block, including `prep()` and `bake()` functions, is for demonstration purposes only. Both functions are performed internally when `workflows()` is applied later in subsequent steps.
```{r}
# Prep only on training data
dep_prep <- prep(dep_recipe, training = train)
train_prepped <- bake(dep_prep, new_data = NULL)
test_prepped <- bake(dep_prep, new_data = test)
```

But the code block allows for intermediate inspection (i.e., to view the prepped and baked objects), as follows:
```{r}
dep_prep

glimpse(train_prepped)
```


### Model specification & fit (OLS)
```{r}
ols_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")
```


```{r}
ols_wf <- workflow() %>%
  add_recipe(dep_recipe) %>%
  add_model(ols_spec)
```

```{r}
ols_fit <- fit(ols_wf, data = train)
ols_fit
```

### Fit model to the held-out test set

```{r}
# Generate predictions for the test data, and arrange the output in a 2-column table containing the predicted and actual values (of dep_score)
test_preds <- predict(ols_fit, new_data = test) %>%
  bind_cols(test %>% 
              dplyr::select(dep_score))
```


### Evaluate model performance
```{r}
#compute regression metrics
metrics(test_preds, truth = dep_score, estimate = .pred) # general

#alternatively:
metric_set(rmse, rsq, mae)(test_preds, truth = dep_score, estimate = .pred) # more specific
```
Overall, the model is performing rather poorly. Interpretation:

* RMSE (root mean square error) = 4.15: an average error between the predicted and actual values of `dep_score`. That is, the model's typical inaccuracy is an average error of 4.15 points.
* rsq (_R^2^_) = 0.067: the predictors explain only about 6.7% (very little) of the variance in the actual `dep_score` among participants.
* mae (mean absolute error) = 3.01: the average prediction is off by 3 points.


```{r}
# Visualize observed vs predicted
ggplot(test_preds, aes(x = dep_score, y = .pred)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(title = "OLS: Observed vs Predicted (Test Set)",
       x = "Observed dep_score", y = "Predicted dep_score")
```

The funnel-shaped plot suggests severe non-normality / heteroscedasticity. The OLS assumptions seem heavily violated.

#### More model diagnostics:
```{r}
lm_obj <- ols_fit %>% 
  extract_fit_engine()

par(mfrow = c(2, 2))
plot(lm_obj)
par(mfrow = c(1, 1))
```


```{r}
car::vif(lm_obj)                     # Variance Inflation Factors
```



```{r}
cd <- cooks.distance(lm_obj)
plot(cd, type = "h", main = "Cook's Distance")
abline(h = 4/length(cd), col = "red", lty = 2)
```

#### Diagnostics summary:
The Variance Inflation Factors (VIFs) ranged between 1.02 and 1.73, suggesting no concern for multicollinearity among predictors. Cook’s distance values were uniformly low (maximum ≈ 0.012),
indicating no influential outliers. Therefore, the model’s poor fit likely stems from functional form misspecification rather than multicollinearity or leverage effects.


Given these results, I proceeded to transform the dependent variable (first using Box–Cox, then log transformations) to address potential nonlinearity and variance instability, before exploring regularized alternatives.

## Model B — Box-Cox-Transformed Outcome

* Create the recipe:
```{r}
box_recipe <- recipe(
  dep_score ~ sex + age + race_eth + educ + marital + physact + alc_drinks + cig100 + bmi
  + sleep_hrs,  data = train
) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_BoxCox(all_outcomes(), id = "box_dep") %>% # the Box-Cox transformation on dep_score
  step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
  step_zv(all_predictors())
```

* Specify the model:
```{r}
box_spec <- linear_reg() %>% 
  set_engine("lm") %>%
  set_mode("regression")
```

```{r}
box_wf <- workflow() %>% 
  add_recipe(box_recipe) %>% 
  add_model(box_spec)
```

```{r}
box_fit  <- fit(box_wf, data = train)
```

### Quick summary on Model B: Box-Cox-transformed outcome
Attempt to use Box-Cox transformation threw an error message, due to the zero values in `dep_score` (Box-Cox works strictly on positive values). So, I switch to Yeo-Johnson method instead, which is an extension of Box-Cox but handles positive, zero, and negative values. This is done using the `yeojohnson()` inside the `bestNormalize` library.

## Model B.2 — Yeo-Johnson-Transformed Outcome

```{r}
# apply the Yeo-Johnson to the outcome manually
yj_obj <- bestNormalize::yeojohnson(train$dep_score)
train$dep_score_yj <- predict(yj_obj)
test$dep_score_yj <- predict(yj_obj, newdata = test$dep_score)
```

* Create recipe:
```{r}
yj_recipe <- recipe(
  dep_score_yj ~ sex + age + race_eth + educ + marital + physact + alc_drinks + cig100 + bmi
  + sleep_hrs,  data = train
) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
  step_zv(all_predictors())
```


* Workflow + model:
```{r}
yj_spec <- linear_reg() %>% 
  set_engine("lm") %>%
  set_mode("regression")

yj_wf <- workflow() %>% 
  add_recipe(yj_recipe) %>% 
  add_model(yj_spec)

yj_fit  <- fit(yj_wf, data = train)
```


* Fit model to the held-out test set:
```{r}
# Generate predictions for the test data
test_yj_preds <- predict(yj_fit, new_data = test) %>%
  bind_cols(test %>% 
              dplyr::select(dep_score_yj, dep_score))
```

```{r}
# Convert predictions back to original dep_score scale
test_yj_preds <- test_yj_preds %>%
  mutate(
    dep_score_pred_original = predict(yj_obj, newdata = .pred, inverse = TRUE))
```

* Evaluate model performance:
```{r}
#compute regression metrics 
metric_set(rmse, rsq, mae)(
  test_yj_preds, 
  truth = dep_score_yj, 
  estimate = .pred)
```


```{r}
#compute regression metrics for the back-converted (raw) scale
metric_set(rmse, rsq, mae)(
  test_yj_preds, 
  truth = dep_score, 
  estimate = dep_score_pred_original)
```


### Quick summary on Model B.2: Yeo–Johnson-transformed outcome

Applying the Yeo–Johnson transformation improved model behavior on the transformed scale (RMSE = 0.95; R^2^ = 0.077; MAE = 0.83), suggesting better fit and residual stability. However, after converting predictions back to the original PHQ-9 scale, the performance (RMSE = 4.46; R^2^ = 0.069; MAE = 2.81) remained comparable to the raw-outcome model (Model A). This indicates that while the transformation enhances statistical properties, it does not materially improve out-of-sample predictive accuracy in the original units of depression scores. The transformation is therefore most useful for assumption diagnostics rather than predictive gains.


## Model C — Log-transformed Outcome
**Rationale** <br>
The log transformation is applied to reduce right-skew and compress high depression scores, improving linearity and stabilizing residual variance. Unlike Yeo–Johnson, the log transform requires strictly positive values, so a constant of 1 is added to all scores prior to transformation.

```{r}
train$dep_score_log <- log1p(train$dep_score)
test$dep_score_log <- log1p(test$dep_score)
```


```{r}
# Apply log(1 + dep_score) transformation for strictly positive outcome
log_recipe <- recipe(
  dep_score_log ~ sex + age + race_eth + educ + marital + physact +
    alc_drinks + cig100 + bmi + sleep_hrs,
  data = train
) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
  step_zv(all_predictors())
```


* Model specification and fitting
```{r}
log_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

log_wf <- workflow() %>%
  add_recipe(log_recipe) %>%
  add_model(log_spec)

log_fit <- fit(log_wf, data = train)
```

* Predictions and back-transformation
```{r}
# Generate predictions for test data
test_log_preds <- predict(log_fit, new_data = test) %>%
  bind_cols(test %>% 
              dplyr::select(dep_score, dep_score_log))
```


```{r}
# Convert predictions back to original PHQ-9 scale
test_log_preds <- test_log_preds %>%
  mutate(dep_score_pred_original = expm1(.pred))  # inverse of log1p()
```

* Evaluate model performance:
```{r}
#compute regression metrics
metric_set(rmse, rsq, mae)(
  test_log_preds, 
  truth = dep_score_log, 
  estimate = .pred)
```



```{r}
#compute regression metrics for the back-converted (raw) scale
metric_set(rmse, rsq, mae)(
  test_log_preds, 
  truth = dep_score, 
  estimate = dep_score_pred_original)
```

### Quick summary on Model C: Log-transformed outcome

Applying the log-transformation improved model behavior (over those of the untransformed and Yeo–Johnson-transformed outcome) on the transformed scale (RMSE = 0.87; R^2^ = 0.078; MAE = 0.74), suggesting better fit and residual stability. However, after converting predictions back to the original PHQ-9 scale, the performance (RMSE = 4.37; R^2^ = 0.070; MAE = 2.80) remained comparable to the previous models (Models A and B.2). Again, this indicates that while the transformation enhances statistical properties, it does not materially improve out-of-sample predictive accuracy in the original units of depression scores. The transformation is therefore most useful for assumption diagnostics rather than predictive gains.



# Cross-Validation, Regularization, Interpretability, and Final Reflection

## Cross-Validation — OLS Stability & Baseline Comparison
Purpose: estimate how unstable the OLS estimates are to sample splits and produce cross-validated estimates of RMSE/R^2 on the training set.

```{r}
set.seed(58)  
cv_splits <- vfold_cv(train, v = 5, strata = NULL)
```

```{r}
# Fit resamples for the OLS workflow previously defined (ols_wf).
ols_res <- ols_wf %>%
  fit_resamples(
    resamples = cv_splits,
    metrics = metric_set(rmse, rsq, mae),
    control = control_resamples(save_pred = TRUE) # setting this to TRUE allows for later inspection of fold-wise predictions if needed
  )
```

```{r}
# Summary of CV performance (mean RMSE, R^2, and MAE)
cv_metrics <- collect_metrics(ols_res)
cv_metrics
```

The cross-validation results showed that the OLS predictive performance is consistently poor across folds. Most importantly, the mean R^2 of 0.084 suggests that a vast majority (over 91%) of the variability in the outcome remains unexplained by the current model specification. This corroborated the poor performance observed with the held-out test sets (across transformed and untransformed models).


```{r}
# Optional visualization of RMSE distribution across folds:
tryCatch(autoplot(ols_res), error = function(e) cat("Autoplot not available; use collect_metrics()\n"))
```




## Regularized Alternative — Elastic Net (`glmnet`)
Purpose: use regularization to reduce variance, limit overfitting, and possibly recover predictive performance if OLS is noisy.<br>
Notes: Elastic Net mixes L2 (ridge) and L1 (lasso) penalties via `mixture`. I tune penalty and mixture using CV on the training set, then finalize the model and evaluate on the same held-out test set used earlier. I keep the same `dep_recipe` (predictor pre-processing), to allow an 'apples-to-apples' comparison. I also use a log-scaled penalty grid to cover many magnitudes.

* Model specification
```{r}
enet_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet") %>%
  set_mode("regression")
```


```{r}
enet_wf <- workflow() %>%
  add_recipe(dep_recipe) %>%
  add_model(enet_spec)
```

* Hyperparameters and tuning
```{r}
# Better penalty grid: penalty is on regular scale, but we tune on log10 range
penalty_grid <- 10^seq(-5, 1, length.out = 20)  # 20 logarithmically placed values: from 10^-5 (i.e., 0.00001) to 10^1 (i.e., 10)
grid <- expand_grid(
  penalty = penalty_grid,
  mixture = seq(0, 1, length.out = 11) # 11 evenly spaced values: from 0 to 1
)
```


```{r}
doParallel::registerDoParallel()   # speedup for tune_grid
tune_res <- tune_grid(
  enet_wf,
  resamples = cv_splits,
  grid = grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE)
)
```


```{r}
# Inspect tuning results; order by RMSE
show_best(tune_res, metric = "rmse")
```


```{r}
# Inspect tuning results; order by RSQ
show_best(tune_res, metric = "rsq")
```

* Select the best hyperparameter from tuning results:
```{r}
# Select best (lowest RMSE) and finalize workflow
best <- select_best(tune_res, metric = "rmse")
final_enet <- finalize_workflow(enet_wf, best)
```

* Fit selection to data:
```{r}
# Fit final ENet on full training data
final_fit_enet <- fit(final_enet, data = train)
```


```{r}
# Evaluate ENet on held-out test set (use same test used for Models A–C)
test_enet_preds <- predict(final_fit_enet, new_data = test) %>%
  bind_cols(test %>% 
              dplyr::select(dep_score))
```


* Evaluate model performance:
```{r}
enet_test_metrics <- metric_set(rmse, rsq, mae)(
  test_enet_preds, truth = dep_score, estimate = .pred
  )
enet_test_metrics
```

The Elastic Net model showed slightly weaker performance on the test set than the mean cross-validated performance, which is expected when tuning is performed on the training set. The final R^2^ of 6.7% confirms that, although the model is stable and appropriately regularized, regularization cannot compensate for the lack of predictive signal in the available features.  

Because all models achieved very low R^2^ and showed no stable or interpretable signal, variable-importance and coefficient-interpretation sections were intentionally omitted to avoid over-interpreting noise.


### Final Reflection
**Model selection**  
I compared a baseline OLS model (raw outcome), two outcome-transformation variants (Yeo–Johnson and log(1+x)), and an Elastic Net regularized linear model. Transformations improved statistical behavior on transformed scales (more symmetric residuals, reduced heteroscedasticity), but once predictions were back-transformed to the original PHQ-9 scale, all models showed similar predictive performance (R^2^/RMSE/MAE). <br>

**Interpretation & implications:**  
- The poor R^2 (~0.06-0.08) shows that the available predictors explain only a small portion of the variance in depression scores. This reflects a substantive limitation, not a modelling failure.
- Low VIFs and negligible Cook's distances indicate neither multicollinearity nor influential single observations was reponsible for the weak performance; the limitation appears structural (e.g., omitted variables, measurement error, nonlinear relationships, or ceiling/floor effects).
- While the model included several established correlates of depression (age, gender, marital status, education, physical activity, sleep), other important determinants such as income, employment status, and social support were unavailable, which likely contributed to the low predictive signal. <br>

**Recommended next steps:**

1. Feature engineering: Explore nonlinear terms, interactions, spline transformations, and composite survey constructs.
2. Nonlinear learners: Evaluate Random Forests and Gradient Boosting (XGBoost/LightGBM) with proper cross-validation and calibration checks.
3. Multiple Imputation: Use `mice` when missingness patterns justify it and pool estimates if inferential conclusions are required.
4. Interpretability: For more complex models, apply permutation importance, PDPs, accumulated local effects, and SHAP to characterize patterns without over-interpreting linear model coefficients.


## Reproducibility & Session Info
```{r}
sessioninfo::session_info()
```


###### ---End of document---

